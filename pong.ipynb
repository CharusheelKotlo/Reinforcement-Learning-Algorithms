{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charu\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Environment : Pong-v0\n",
      "State space: Box(0, 255, (210, 160, 3), uint8)\n",
      "Action space: Discrete(6)\n",
      "Running random agent...\n",
      "Total reward after 1058 steps: -21.0\n",
      "\n",
      "These are the Rewards(and respective frequencies) after experienting with Random Agent : \n",
      " {0.0: 1037, -1.0: 21}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"uncomment it in case gynmasium is not present\"\"\"\n",
    "\n",
    "# %pip install gymnasium[all]\n",
    "# %pip install shimmy\n",
    "# %pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "\n",
    "\"\"\"uncomment it to check if Pong-v0 and others are present or not\"\"\"\n",
    "# # To Check if all modules are working fine \n",
    "# import ale_py\n",
    "# import gymnasium\n",
    "# import shimmy\n",
    "\n",
    "# import gymnasium as gym\n",
    "\n",
    "# req = set(['MountainCar-v0','Pong-v0','CartPole-v0','LunarLander-v2'])\n",
    "# keys = set(gym.envs.registry.keys())\n",
    "# for key in req:\n",
    "\n",
    "#   if(keys.intersection({key}) == set({})):\n",
    "#       print(key +' not found')   \n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import shimmy #it is compulsory to import\n",
    "import ale_py #it's too\n",
    "\n",
    "env_1 = gym.make(\"Pong-v0\")\n",
    "print('\\n Environment : Pong-v0')\n",
    " # Print state and action space\n",
    "print(\"State space:\", env_1.observation_space)\n",
    "print(\"Action space:\", env_1.action_space)\n",
    "state, _ = env_1.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "rew_1 = {}\n",
    "print(\"Running random agent...\")\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = env_1.action_space.sample()  # Random action\n",
    "    state, reward, done, _, _ = env_1.step(action)\n",
    "    total_reward += reward\n",
    "    if reward not in rew_1:\n",
    "        rew_1[reward] = 0\n",
    "    rew_1[reward] += 1\n",
    "    step_count += 1\n",
    "\n",
    "print(f\"Total reward after {step_count} steps: {total_reward}\\n\")\n",
    "\n",
    "# Close the environment\n",
    "env_1.close()\n",
    "print(f'These are the Rewards(and respective frequencies) after experienting with Random Agent : \\n {rew_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Environment : Pong-v0\n",
      "State space: Box(0, 255, (128,), uint8)\n",
      "Action space: Discrete(6)\n",
      "Running random agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charu\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment Pong-ram-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after 1108 steps: -21.0\n",
      "\n",
      "These are the Rewards(and their frequencies) after experienting with Random Agent \n",
      " {0.0: 1087, -1.0: 21}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "env_2 = gym.make(\"Pong-ram-v0\")\n",
    "print('\\n Environment : Pong-v0')\n",
    " # Print state and action space\n",
    "print(\"State space:\", env_2.observation_space)\n",
    "print(\"Action space:\", env_2.action_space)\n",
    "state, _ = env_2.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "rew_2={}\n",
    "allowed_actions = [0,2,3]\n",
    "print(\"Running random agent...\")\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = random.choice(allowed_actions)  # Random action\n",
    "    state, reward, done, _, _ = env_2.step(action)\n",
    "    total_reward += reward\n",
    "    if reward not in rew_2:\n",
    "        rew_2[reward] = 0\n",
    "    rew_2[reward] += 1\n",
    "    step_count += 1\n",
    "\n",
    "print(f\"Total reward after {step_count} steps: {total_reward}\\n\")\n",
    "\n",
    "# Close the environment\n",
    "env_2.close()\n",
    "\n",
    "print(f'These are the Rewards(and their frequencies) after experienting with Random Agent \\n {rew_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "ENVIRONMENT = \"Pong-v0\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = 'cpu'\n",
    "\n",
    "SAVE_MODELS = True  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./pong-cnn-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 50  # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = True  # Load model from file\n",
    "LOAD_FILE_EPISODE = 500  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 32  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 1500  # Max episode\n",
    "MAX_STEP = 2000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 20000  # Max memory len\n",
    "MIN_MEMORY_LEN = 1000  # Min memory len before start train -----------------------------\n",
    "\n",
    "GAMMA = 0.99  # Discount rate\n",
    "ALPHA = 0.00025  # Learning rate\n",
    "EPSILON_DECAY = 0.995  # Epsilon decay rate by step\n",
    "\n",
    "\n",
    "RENDER_GAME_WINDOW = False  # Opens a new window to render the game (Won't work on colab default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQNPong(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQNPong, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            # nn.LeakyReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            # nn.LeakyReLU()\n",
    "            nn.Conv2d(4,16,8,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,4,2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*9*9, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor before passing it to fully connected layers\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "        # State size for breakout env. SS images (210, 160, 3). Used as input size in network\n",
    "        self.state_size_h = environment.observation_space.shape[0]\n",
    "        self.state_size_w = environment.observation_space.shape[1]\n",
    "        self.state_size_c = environment.observation_space.shape[2]\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Image pre process params\n",
    "        self.target_h = 84  # Height after process\n",
    "        self.target_w = 84  # Width after process\n",
    "\n",
    "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]  # Cut 20 px from top to get rid of the score table\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        # After many experinces epsilon will be 0.05\n",
    "        # So we will do less Explore more Exploit\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        # Deque holds replay mem.\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DQNPong(self.action_size).to(DEVICE) #------------change here\n",
    "        self.target_model = DQNPong(self.action_size).to(DEVICE) #------------change here\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha,weight_decay=1e-5)\n",
    "\n",
    "    def preProcess(self, image):\n",
    "        \"\"\"\n",
    "        Process image crop resize, grayscale and normalize the images\n",
    "        \"\"\"\n",
    "\n",
    "        # print(type(image))\n",
    "        # image = image[0]\n",
    "        frame = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)  # To grayscale\n",
    "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]  # Cut 20 px from top\n",
    "        frame = cv2.resize(frame, (self.target_w, self.target_h))  # Resize\n",
    "        frame = frame.reshape(self.target_w, self.target_h) / 255  # Normalize\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Get state and do action\n",
    "        Two option can be selectedd if explore select random action\n",
    "        if exploit ask nnet for action\n",
    "        \"\"\"\n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        if len(self.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            return loss, max_q\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "\n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        # Get indice of the max value of next_states_q_values\n",
    "        # Use that indice to get a q_value from next_states_target_q_values\n",
    "        # We use greedy for policy So it called off-policy\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "\n",
    "        # Calc loss with expected_q_value and q_value\n",
    "        # loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(selected_q_value, expected_q_value)\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\charu\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\charu\\AppData\\Local\\Temp\\ipykernel_10720\\2318296940.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pth\"))\n",
      " 10%|\u001b[32m█         \u001b[0m| 100/999 [47:15<7:41:42, 30.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:600  Reward:-17.00 Loss:13.33 Last_100_Avg_Rew:-16.960 Avg_Max_Q:1.347 Epsilon:0.05 Step:2599 CStep:216904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|\u001b[32m██        \u001b[0m| 200/999 [1:37:15<6:28:23, 29.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:700  Reward:-16.00 Loss:14.89 Last_100_Avg_Rew:-16.500 Avg_Max_Q:1.741 Epsilon:0.05 Step:2414 CStep:450255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|\u001b[32m███       \u001b[0m| 300/999 [2:27:18<5:57:49, 30.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:800  Reward:-19.00 Loss:14.16 Last_100_Avg_Rew:-16.220 Avg_Max_Q:1.783 Epsilon:0.05 Step:2573 CStep:687726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|\u001b[32m████      \u001b[0m| 400/999 [3:16:24<5:11:46, 31.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:900  Reward:-11.00 Loss:16.94 Last_100_Avg_Rew:-16.950 Avg_Max_Q:1.678 Epsilon:0.05 Step:2773 CStep:919038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|\u001b[32m█████     \u001b[0m| 500/999 [4:07:19<4:17:22, 30.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1000  Reward:-20.00 Loss:12.35 Last_100_Avg_Rew:-16.340 Avg_Max_Q:1.803 Epsilon:0.05 Step:2405 CStep:1159180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|\u001b[32m██████    \u001b[0m| 600/999 [4:57:39<3:12:58, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1100  Reward:-14.00 Loss:14.18 Last_100_Avg_Rew:-16.830 Avg_Max_Q:1.739 Epsilon:0.05 Step:2404 CStep:1396641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|\u001b[32m███████   \u001b[0m| 700/999 [5:48:39<2:22:05, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1200  Reward:-15.00 Loss:12.61 Last_100_Avg_Rew:-16.640 Avg_Max_Q:1.696 Epsilon:0.05 Step:2408 CStep:1636154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|\u001b[32m████████  \u001b[0m| 800/999 [6:40:37<1:42:19, 30.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1300  Reward:-18.00 Loss:11.79 Last_100_Avg_Rew:-15.680 Avg_Max_Q:1.743 Epsilon:0.05 Step:2100 CStep:1881674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|\u001b[32m█████████ \u001b[0m| 900/999 [7:31:18<52:48, 32.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1400  Reward:-16.00 Loss:15.39 Last_100_Avg_Rew:-15.680 Avg_Max_Q:1.777 Epsilon:0.05 Step:2982 CStep:2123682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████\u001b[0m| 999/999 [8:22:23<00:00, 30.17s/it]\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(ENVIRONMENT)  # Get env\n",
    "agent = Agent(environment)  # Create Agent\n",
    "\n",
    "if LOAD_MODEL_FROM_FILE:\n",
    "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pth\"))\n",
    "\n",
    "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
    "        param = json.load(outfile)\n",
    "        agent.epsilon = param.get('epsilon')\n",
    "\n",
    "    startEpisode = LOAD_FILE_EPISODE + 1\n",
    "\n",
    "else:\n",
    "    startEpisode = 1\n",
    "\n",
    "last_100_ep_reward = deque(maxlen=100)  # Last 100 episode rewards\n",
    "total_step = 1  # Cumulkative sum of all steps in episodes\n",
    "for episode in tqdm(range(startEpisode, MAX_EPISODE),colour='green'):\n",
    "\n",
    "    # startTime = time.time()  # Keep time\n",
    "    state,_ = environment.reset()  # Reset env\n",
    "    \n",
    "    state = agent.preProcess(state)  # Process image\n",
    "\n",
    "    # Stack state . Every state contains 4 time contionusly frames\n",
    "    # We stack frames like 4 channel image\n",
    "    state = np.stack((state, state, state, state))\n",
    "\n",
    "    total_max_q_val = 0  # Total max q vals\n",
    "    total_reward = 0  # Total reward for each episode\n",
    "    total_loss = 0  # Total loss for each episode\n",
    "    done = False\n",
    "    step = 0\n",
    "    while(not done):\n",
    "\n",
    "        if RENDER_GAME_WINDOW:\n",
    "            environment.render()  # Show state visually\n",
    "\n",
    "        step+=1\n",
    "        # Select and perform an action\n",
    "        action = agent.act(state)  # Act\n",
    "        next_state, reward, done,_,_ = environment.step(action)  # Observe\n",
    "        # print(next_state.shape)\n",
    "        next_state = agent.preProcess(next_state)  # Process image\n",
    "\n",
    "        # Stack state . Every state contains 4 time contionusly frames\n",
    "        # We stack frames like 4 channel image\n",
    "        next_state = np.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "        # Store the transition in memory\n",
    "        agent.storeResults(state, action, reward, next_state, done)  # Store to mem\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state  # Update state\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            # Perform one step of the optimization (on the target network)\n",
    "            loss, max_q_val = agent.train()  # Train with random BATCH_SIZE state taken from mem\n",
    "        else:\n",
    "            loss, max_q_val = [0, 0]\n",
    "\n",
    "        total_loss += loss\n",
    "        total_max_q_val += max_q_val\n",
    "        total_reward += reward\n",
    "        total_step += 1\n",
    "        if total_step % 1000 == 0:\n",
    "            agent.adaptiveEpsilon()  # Decrase epsilon\n",
    "\n",
    "        if TRAIN_MODEL and step%200==0:\n",
    "                agent.target_model.load_state_dict(agent.online_model.state_dict())  # Update target model\n",
    "        \n",
    "        if done:  # Episode completed\n",
    "            \n",
    "            epsilonDict = {'epsilon': agent.epsilon}  # Create epsilon dict to save model as file\n",
    "\n",
    "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # Save model as file\n",
    "                weightsPath = MODEL_PATH + str(episode) + '.pth'\n",
    "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
    "\n",
    "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
    "                with open(epsilonPath, 'w') as outfile:\n",
    "                    json.dump(epsilonDict, outfile)\n",
    "\n",
    "            last_100_ep_reward.append(total_reward)\n",
    "            avg_max_q_val = total_max_q_val / step\n",
    "\n",
    "            outStr = \"Episode:{}  Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Step:{} CStep:{}\".format(\n",
    "                episode, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon,  step, total_step\n",
    "            )\n",
    "            if(episode%100==0):\n",
    "                print(outStr)\n",
    "            if episode%10==0:\n",
    "                if SAVE_MODELS:\n",
    "                    outputPath = MODEL_PATH + \"out\" + '.csv'  # Save outStr to file\n",
    "                    with open(outputPath, 'a') as outfile:\n",
    "                        outfile.write(f'{episode},{total_reward},{total_loss},{np.mean(last_100_ep_reward)}, {avg_max_q_val}, {agent.epsilon},  {step}, {total_step}'+\"\\n\")\n",
    "\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
