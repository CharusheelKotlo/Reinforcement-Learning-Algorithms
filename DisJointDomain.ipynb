{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":87050,"databundleVersionId":9875293,"sourceType":"competition"},{"sourceId":9684508,"sourceType":"datasetVersion","datasetId":5919969},{"sourceId":143469,"sourceType":"modelInstanceVersion","modelInstanceId":121555,"modelId":144694}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.autograd import Function\nfrom torchvision.models.vision_transformer import vit_b_16\nfrom torchvision.models import ResNet152_Weights\nfrom torchvision.models import ViT_B_16_Weights\n!pip install peft\nimport torch\nimport os\nfrom transformers import ViTForImageClassification\nfrom peft import LoraConfig,LoraModel\nfrom transformers import ViTForImageClassification\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:30:59.558618Z","iopub.execute_input":"2024-10-24T11:30:59.558987Z","iopub.status.idle":"2024-10-24T11:31:18.693398Z","shell.execute_reply.started":"2024-10-24T11:30:59.558951Z","shell.execute_reply":"2024-10-24T11:31:18.692480Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e939c7975e14de59d709fa32c30bf00"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:31:30.617933Z","iopub.execute_input":"2024-10-24T11:31:30.618486Z","iopub.status.idle":"2024-10-24T11:31:30.641388Z","shell.execute_reply.started":"2024-10-24T11:31:30.618442Z","shell.execute_reply":"2024-10-24T11:31:30.640475Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"490f6e3ea1f249809f299e12761ee790"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"class ModifiedViTForImageClassification(ViTForImageClassification):\n    def forward(self, pixel_values, **kwargs):\n        outputs = self.vit(pixel_values, **kwargs)\n        return outputs[0][:,0]  # Returns class token as output\n    \n# Define the Gradient Reversal Layer (GRL)\nclass GradientReversalLayer(Function):\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n        return x.view_as(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output.neg() * ctx.alpha\n        return output, None\n\n# Contrastive Loss (Cosine Similarity)\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n    \n    def forward(self, features_a, features_b, label):\n        cosine_sim = F.cosine_similarity(features_a, features_b)\n        loss = (1 - label) * 0.5 * torch.pow(cosine_sim, 2) + \\\n               label * 0.5 * torch.pow(torch.clamp(self.margin - cosine_sim, min=0.0), 2)\n        return loss.mean()\n\n# Custom model with pretrained ResNet and ViT with GRL\nclass CustomResNetPretrainedViTDANN(nn.Module):\n    def __init__(self, num_classes=18, num_domains=4, vit_dim=768, dropout_p=0.5):  # ViT base has output dim of 768\n        super(CustomResNetPretrainedViTDANN, self).__init__()\n        \n        # Load the pretrained ResNet\n        self.resnet = models.resnet152(weights=ResNet152_Weights.DEFAULT)\n        for param in self.resnet.parameters():\n            param.requires_grad = False  # Freeze ResNet parameters\n            \n        # Keep the last layer trainable\n        for param in self.resnet.layer4.parameters():\n            param.requires_grad = True\n            \n        for param in self.resnet.layer3.parameters():\n            param.requires_grad = True\n        \n        self.resnet.fc = nn.Identity()  # Remove the classification head to output features\n\n        self.resnet_to_vit_projection = nn.Linear(2048, vit_dim)\n\n        # Instance Normalization for ResNet features\n        self.instance_norm_resnet = nn.InstanceNorm1d(vit_dim)\n\n        # Load pretrained Vision Transformer (ViT)\n#         self.vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n#         self.vit_to_projection = nn.Linear(1000, vit_dim)\n        \n        model_name = \"google/vit-base-patch16-224\"\n        config = LoraConfig(\n                    task_type=\"CAUSAL_LM\",\n                    r=3,\n                    lora_alpha=17,\n                    target_modules=['query','key','value','dense'],\n                    lora_dropout=0.01,\n                    )\n        modified_model = ModifiedViTForImageClassification.from_pretrained(model_name)\n        self.lora_model = LoraModel(modified_model, config, \"default\")\n\n        # Instance Normalization for ViT features\n        self.instance_norm_vit = nn.InstanceNorm1d(vit_dim)\n\n        # Freeze all ViT layers except the last two (LayerNorm and Classifier)\n#         for name, param in self.vit.named_parameters():\n# #             if 'heads' in name or 'norm' in name:\n# #                 param.requires_grad = True  # Make last two layers trainable\n# #             else:\n# #                 param.requires_grad = False  # Freeze the rest of the ViT\n#             param.requires_grad = True\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(p=dropout_p)\n\n        # Classifier heads\n        self.class_classifier = nn.Sequential(\n            nn.Linear(vit_dim * 2, vit_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),  # Add dropout in the classifier\n            nn.Linear(vit_dim, num_classes),\n            nn.Softmax(dim=1)\n        )\n\n        self.domain_classifier = nn.Sequential(\n            nn.Linear(vit_dim * 2, vit_dim),\n            nn.ReLU(),\n            nn.Dropout(p=dropout_p),  # Add dropout in the domain classifier\n            nn.Linear(vit_dim, num_domains),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x, alpha=1.0):\n        # Extract features using pretrained ResNet\n        resnet_features = self.resnet(x)\n        resnet_features = resnet_features.view(resnet_features.size(0), -1)  # Flatten ResNet output\n        resnet_features = self.resnet_to_vit_projection(resnet_features)  # Project ResNet features to match ViT dimensions\n#         print(resnet_features.shape)\n\n        # Instance normalization for ResNet features\n        resnet_features = self.instance_norm_resnet(resnet_features)\n\n        # Extract features using pretrained ViT\n        vit_features = self.lora_model(x)\n\n        # Instance normalization for ViT features\n        vit_features = self.instance_norm_vit(vit_features)\n\n        # Concatenate ResNet and ViT features\n        combined_features = torch.cat((resnet_features, vit_features), dim=1)  # Concatenate along feature dimension\n\n        # Apply dropout for regularization after concatenation\n        combined_features = self.dropout(combined_features)\n\n        # Classification output\n        class_output = self.class_classifier(combined_features)\n\n        # Apply GRL for domain classification\n        reverse_features = GradientReversalLayer.apply(combined_features, alpha)\n        domain_output = self.domain_classifier(reverse_features)\n\n        return class_output, domain_output, combined_features\n\n\n\n# Entropy minimization loss for domain classifier\ndef entropy_minimization_loss(predictions):\n    epsilon = 1e-8  # To avoid log(0)\n    entropy_loss = -torch.mean(torch.sum(predictions * torch.log(predictions + epsilon), dim=1))\n    return entropy_loss\n\n# Create the model\ndef create_custom_resnet_pretrained_vit_dann(num_classes=18, num_domains=4, vit_dim=768):\n    return CustomResNetPretrainedViTDANN(num_classes=num_classes, num_domains=num_domains, vit_dim=vit_dim)\n\n# Example usage\nif __name__ == \"__main__\":\n    model = create_custom_resnet_pretrained_vit_dann(num_classes=18, num_domains=4)\n    \n    # Example images (random for demonstration)\n    img1 = torch.randn(1, 3, 224, 224)  # Example input image\n\n    # Forward pass through the model\n    class_output, domain_output, features = model(img1, alpha=1.0)\n    \n    # Simulated pairs for contrastive loss (random)\n    features_a = features\n    features_b = torch.randn_like(features)  # Example paired feature set\n    contrastive_labels = torch.randint(0, 2, (features.size(0),))  # 0 or 1 labels for contrastive loss\n\n    # Contrastive loss\n    contrastive_criterion = ContrastiveLoss()\n    contrastive_loss = contrastive_criterion(features_a, features_b, contrastive_labels)\n\n    # Entropy minimization on domain output\n    entropy_loss = entropy_minimization_loss(domain_output)\n\n    print(\"Class Output shape:\", class_output.shape)  # Should be [1, 18]\n    print(\"Domain Output shape:\", domain_output.shape)  # Should be [1, 4]\n    print(\"Contrastive Loss:\", contrastive_loss.item())\n    print(\"Entropy Loss:\", entropy_loss.item())\n    print(features.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:31:41.443121Z","iopub.execute_input":"2024-10-24T11:31:41.443504Z","iopub.status.idle":"2024-10-24T11:31:49.084957Z","shell.execute_reply.started":"2024-10-24T11:31:41.443465Z","shell.execute_reply":"2024-10-24T11:31:49.084098Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n100%|██████████| 230M/230M [00:03<00:00, 78.6MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046a0b98e04d4a20b38f4a10ea751198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6727b33410524cc79b7ded2dc0f6ec3d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n","output_type":"stream"},{"name":"stdout","text":"Class Output shape: torch.Size([1, 18])\nDomain Output shape: torch.Size([1, 4])\nContrastive Loss: 0.5033111572265625\nEntropy Loss: 1.1977946758270264\ntorch.Size([1, 1536])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import datasets, transforms,ops\nfrom PIL import Image\n\n# Custom dataset to handle both class and domain labels, excluding the test folder\nclass DomainClassDataset(Dataset):\n    def __init__(self, root_dir, exclude_domain='test', transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.class_labels = []\n        self.domain_labels = []\n        self.domain_mapping = {}\n        self.class_mapping = {}\n\n        # Populate the image paths and labels, excluding the test domain\n        for domain_idx, domain in enumerate(sorted(os.listdir(root_dir))):\n            if domain == exclude_domain:  # Skip the test folder\n                continue\n            self.domain_mapping[domain] = domain_idx  # Assign a domain index\n            domain_path = os.path.join(root_dir, domain)\n\n            if os.path.isdir(domain_path):\n                for class_name in sorted(os.listdir(domain_path)):\n                    class_path = os.path.join(domain_path, class_name)\n                    if os.path.isdir(class_path):\n                        if class_name not in self.class_mapping:\n                            self.class_mapping[class_name] = len(self.class_mapping)  # Assign a unique ID to each class\n                        for img_name in os.listdir(class_path):\n                            img_path = os.path.join(class_path, img_name)\n                            if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n                                self.image_paths.append(img_path)\n                                self.class_labels.append(self.class_mapping[class_name])\n                                self.domain_labels.append(self.domain_mapping[domain])\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        class_label = self.class_labels[idx]\n        domain_label = self.domain_labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, class_label, domain_label\n\n# Data transformation (normalization, etc.)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),  # Resize with bilinear interpolation\n    transforms.RandomHorizontalFlip(),          # Randomly flip horizontally\n    transforms.ColorJitter(                     # Randomly change brightness, contrast, saturation, and hue\n        brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2\n    ),\n    transforms.RandomGrayscale(p=0.1),          # Convert to grayscale with a probability\n    transforms.RandomApply([                    # Randomly apply Gaussian blur\n        transforms.GaussianBlur(kernel_size=(3, 3))], p=0.2),\n    transforms.RandomRotation(degrees=15),      # Random rotation of up to 15 degrees\n    transforms.ToTensor()  ,                    # Convert the image to a PyTorch tensor\n    transforms.Normalize(                       # Normalize with ImageNet mean and std\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) \n])\n\n\n# Create the dataset for training and validation\ndataset_root = '/kaggle/input/tidel-hack/tidel/dataset'\nfull_dataset = DomainClassDataset(root_dir=dataset_root, transform=transform)\n\n# Train-validation split (80% train, 20% validation)\ntrain_size = int(0.9 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n# DataLoader for training and validation\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n\n# Load the test dataset, which contains only images (no class folders)\nclass TestDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = [os.path.join(root_dir, img_name) for img_name in os.listdir(root_dir) \n                            if img_name.lower().endswith(('.png', '.jpg', '.jpeg'))]\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n\n        return image, img_path  # Returning image path for reference\n\n# Create the test dataset and DataLoader\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),  # Resize with bilinear interpolation\n    transforms.ToTensor(),\n    transforms.Normalize(                       # Normalize with ImageNet mean and std\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ) \n])\ntest_dataset = TestDataset(root_dir=os.path.join(dataset_root, 'test'), transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n\n# Debugging: Check the dataset information\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")\nprint(f\"Number of classes: {len(full_dataset.class_mapping)}\")\nprint(f\"Number of domains: {len(full_dataset.domain_mapping)}\")\nprint(f\"Number of test samples: {len(test_dataset)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:31:58.413716Z","iopub.execute_input":"2024-10-24T11:31:58.414128Z","iopub.status.idle":"2024-10-24T11:32:00.954055Z","shell.execute_reply.started":"2024-10-24T11:31:58.414090Z","shell.execute_reply":"2024-10-24T11:32:00.953075Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of training samples: 3939\nNumber of validation samples: 438\nNumber of classes: 18\nNumber of domains: 4\nNumber of test samples: 15620\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def generate_adversarial_examples(model, images, labels, epsilon=0.01):\n    images.requires_grad = True\n    \n    # Forward pass to get predictions\n    outputs, _, _ = model(images)\n    loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n    \n    # Backward pass to calculate gradients\n    model.zero_grad()\n    loss.backward()\n    \n    # Generate adversarial images by adding a small perturbation\n    adv_images = images + epsilon * images.grad.sign()\n    adv_images = torch.clamp(adv_images, 0, 1)  # Ensure pixel values are valid\n    \n    return adv_images\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:32:05.966376Z","iopub.execute_input":"2024-10-24T11:32:05.966784Z","iopub.status.idle":"2024-10-24T11:32:05.973506Z","shell.execute_reply.started":"2024-10-24T11:32:05.966743Z","shell.execute_reply":"2024-10-24T11:32:05.972271Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport os\nfrom sklearn.metrics import f1_score\n\n# Set CUDA_LAUNCH_BLOCKING=1 to enable synchronous CUDA calls for debugging\n# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n\n# Assuming ContrastiveLoss and EntropyMinimizationLoss are defined somewhere in your code\nclass ContrastiveLoss(nn.Module):\n    def __init__(self, temperature=0.5):\n        super(ContrastiveLoss, self).__init__()\n        self.temperature = temperature\n\n    def forward(self, z_i, z_j):\n        # Assuming z_i and z_j are embeddings from positive/negative pairs\n        batch_size = z_i.shape[0]\n        z_i = nn.functional.normalize(z_i, dim=1)\n        z_j = nn.functional.normalize(z_j, dim=1)\n        \n        # Positive similarity\n        positive_sim = torch.exp(torch.sum(z_i * z_j, dim=-1) / self.temperature)\n        \n        # All similarities\n        similarities = torch.exp(torch.mm(z_i, z_j.t()) / self.temperature)\n        negative_sim = similarities.sum(dim=-1)\n        \n        loss = -torch.log(positive_sim / negative_sim)\n        return loss.mean()\n\nclass EntropyMinimizationLoss(nn.Module):\n    def __init__(self):\n        super(EntropyMinimizationLoss, self).__init__()\n\n    def forward(self, class_outputs):\n        # Class_outputs is the predicted probability distributions (after softmax)\n        p = torch.softmax(class_outputs, dim=1)\n        log_p = torch.log(p + 1e-10)\n        entropy = -torch.sum(p * log_p, dim=1)\n        return entropy.mean()\n\ndef train_dann_with_adversarial(model, train_loader, val_loader, num_epochs=20, alpha=1.0, learning_rate=0.0001, epsilon=0.01):\n    # Define loss functions\n    classification_criterion = torch.nn.CrossEntropyLoss()\n    domain_criterion = torch.nn.CrossEntropyLoss()\n    contrastive_loss_fn = ContrastiveLoss(temperature=0.5)\n    entropy_loss_fn = EntropyMinimizationLoss()\n\n    # Optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n    \n    # Training Loop\n    for epoch in range(num_epochs):\n        model.train()\n        running_class_loss = 0.0\n        running_domain_loss = 0.0\n        running_contrastive_loss = 0.0\n        running_entropy_loss = 0.0\n        running_corrects = 0\n        total_samples = 0\n\n        # To store true and predicted labels\n        all_preds = []\n        all_labels = []\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\" * 20)\n        \n        for images, class_labels, domain_labels in tqdm(train_loader, desc=\"Training\"):\n            images = images.cuda()  # Transfer to GPU if available\n            class_labels = class_labels.cuda()\n            domain_labels = domain_labels.cuda()\n        \n            optimizer.zero_grad()\n\n            # Forward pass on clean data\n            class_outputs, domain_outputs, embeddings = model(images, alpha=alpha)\n            \n            # Example for contrastive loss - This should be positive and negative pairs\n            z_i, z_j = embeddings, embeddings  # This should be positive/negative pairs (adapt as per your data)\n            contrastive_loss = contrastive_loss_fn(z_i, z_j)\n\n            # Compute entropy minimization loss\n            entropy_loss = entropy_loss_fn(class_outputs)\n\n            # Compute classification and domain losses for clean data\n            class_loss = classification_criterion(class_outputs, class_labels)#---------------\n            # print(class_outputs.shape)\n            # print(class_labels.shape)\n            \n            class_lab = torch.zeros_like(class_outputs).cuda()\n            class_lab.scatter_(1, class_labels.unsqueeze(1), 1)\n\n            domain_lab = torch.zeros_like(domain_outputs).cuda()\n            domain_lab.scatter_(1, domain_labels.unsqueeze(1), 1)\n            \n#             class_loss = ops.sigmoid_focal_loss(class_outputs, class_lab,alpha = 0.25,gamma=1,reduction='sum')\n            domain_loss = domain_criterion(domain_outputs, domain_labels)#--------------\n            \n            \n#             domain_loss = ops.sigmoid_focal_loss(domain_outputs, domain_lab,alpha = 0.25,gamma=1,reduction='sum')\n\n            # Adversarial training\n            adv_images = generate_adversarial_examples(model, images, class_labels, epsilon)\n            \n            # Forward pass on adversarial examples\n            adv_class_outputs, adv_domain_outputs, adv_embeddings = model(adv_images, alpha=alpha)\n            \n            # Compute classification and domain losses for adversarial examples\n            adv_class_loss = classification_criterion(adv_class_outputs, class_labels) #---------------\n#             adv_class_loss = ops.sigmoid_focal_loss(adv_class_outputs, class_lab,alpha = 0.25,gamma=1,reduction='sum')\n            adv_domain_loss = domain_criterion(adv_domain_outputs, domain_labels) #-------------\n#             adv_domain_loss = ops.sigmoid_focal_loss(adv_domain_outputs, domain_lab,alpha = 0.25,gamma=1,reduction='sum')\n            \n            \n            # Total loss: combine clean and adversarial losses with weights\n            total_class_loss = 0.5 * (class_loss + adv_class_loss)\n            total_domain_loss = 0.5 * (domain_loss + adv_domain_loss)\n            total_loss = total_class_loss + pow(3, total_domain_loss) + 0.4 * contrastive_loss + 0.2 * entropy_loss\n\n            # Backward pass and optimization\n            total_loss.backward()\n            optimizer.step()\n\n            # Track loss and accuracy\n            running_class_loss += total_class_loss.item() * images.size(0)\n            running_domain_loss += total_domain_loss.item() * images.size(0)\n            running_contrastive_loss += contrastive_loss.item() * images.size(0)\n            running_entropy_loss += entropy_loss.item() * images.size(0)\n            \n            # Predictions and true labels\n            _, preds = torch.max(class_outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(class_labels.cpu().numpy())\n            running_corrects += torch.sum(preds == class_labels.data)\n            total_samples += images.size(0)\n\n        epoch_class_loss = running_class_loss / total_samples\n        epoch_domain_loss = running_domain_loss / total_samples\n        epoch_entropy_loss = running_entropy_loss / total_samples\n        epoch_acc = running_corrects.double() / total_samples\n\n        # Calculate F1-score for the epoch\n        epoch_f1_score = f1_score(all_labels, all_preds, average='weighted')\n\n        print(f\"Training - Classification Loss: {epoch_class_loss:.4f}, Domain Loss: {epoch_domain_loss:.4f}, Entropy Loss: {epoch_entropy_loss:.4f}, Accuracy: {epoch_acc:.4f}, F1-score: {epoch_f1_score:.4f}\")\n\n        # Validation phase\n        val_class_loss, val_domain_loss, val_acc, val_f1 = evaluate_dann(model, val_loader, classification_criterion, domain_criterion, alpha)\n        print(f\"Validation - Classification Loss: {val_class_loss:.4f}, Domain Loss: {val_domain_loss:.4f}, Accuracy: {val_acc:.4f}, F1-score: {val_f1:.4f}\")\n\n        # Save model every 2 epochs\n        if (epoch + 1) % 2 == 0:\n            save_dir = \"/kaggle/working/\"\n            os.makedirs(save_dir, exist_ok=True)\n            torch.save(model.state_dict(), os.path.join(save_dir, f'model_epoch_{epoch + 1}.pth'))\n            print(f'Model saved at epoch {epoch + 1}.')\n\n\n\n# Evaluation function for validation\ndef evaluate_dann(model, val_loader, classification_criterion, domain_criterion, alpha):\n    model.eval()\n    running_class_loss = 0.0\n    running_domain_loss = 0.0\n    running_corrects = 0\n    total_samples = 0\n\n    # To store true and predicted labels\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, class_labels, domain_labels in tqdm(val_loader, desc=\"Validation\"):\n            images = images.cuda()  # Transfer to GPU if available\n            class_labels = class_labels.cuda()\n            domain_labels = domain_labels.cuda()\n\n            # Forward pass\n            class_outputs, domain_outputs, embeddings = model(images, alpha=alpha)\n\n            # Calculate losses\n            class_loss = classification_criterion(class_outputs, class_labels)\n            domain_loss = domain_criterion(domain_outputs, domain_labels)\n\n            # Track loss and accuracy\n            running_class_loss += class_loss.item() * images.size(0)\n            running_domain_loss += domain_loss.item() * images.size(0)\n            _, preds = torch.max(class_outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(class_labels.cpu().numpy())\n            running_corrects += torch.sum(preds == class_labels.data)\n            total_samples += images.size(0)\n\n    epoch_class_loss = running_class_loss / total_samples\n    epoch_domain_loss = running_domain_loss / total_samples\n    epoch_acc = running_corrects.double() / total_samples\n\n    # Calculate F1-score for validation\n    val_f1_score = f1_score(all_labels, all_preds, average='weighted')\n\n    return epoch_class_loss, epoch_domain_loss, epoch_acc, val_f1_score\n\n# Example usage\n# if __name__ == \"__main__\":\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     model = create_custom_resnet_dann(num_classes=18, num_domains=4).to(device)  # Move the model to GPU if available\n#     model = create_pretrained_resnet_grl(num_classes=18, num_domains=4).to(device)\nmodel = create_custom_resnet_pretrained_vit_dann(num_classes=18, num_domains=4).to(device)\nprint(device)\nnum_gpus = torch.cuda.device_count()\n#     print(f\"Number of GPUs: {num_gpus}\")\nmodel = nn.DataParallel(model)\n#     model = torch.nn.DataParallel(model, device_ids=[0, 1]).to(device)\n#     model = model.to(device)\n# train_dann(model, train_loader, val_loader, num_epochs=20, alpha=1.0, learning_rate=0.00001)\ntrain_dann_with_adversarial(model, train_loader, val_loader, num_epochs=20, alpha=1.0, learning_rate=0.00001)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T11:32:08.696316Z","iopub.execute_input":"2024-10-24T11:32:08.696880Z","iopub.status.idle":"2024-10-24T13:36:34.133100Z","shell.execute_reply.started":"2024-10-24T11:32:08.696827Z","shell.execute_reply":"2024-10-24T13:36:34.132059Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\nEpoch 1/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:01<00:00,  2.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.8594, Domain Loss: 1.3669, Entropy Loss: 2.8897, Accuracy: 0.2110, F1-score: 0.1732\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:08<00:00,  1.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.8126, Domain Loss: 1.3226, Accuracy: 0.4064, F1-score: 0.3177\nEpoch 2/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:06<00:00,  2.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.7046, Domain Loss: 1.3080, Entropy Loss: 2.8830, Accuracy: 0.4184, F1-score: 0.3150\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.6474, Domain Loss: 1.2428, Accuracy: 0.4909, F1-score: 0.3765\nModel saved at epoch 2.\nEpoch 3/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:05<00:00,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.5798, Domain Loss: 1.2213, Entropy Loss: 2.8749, Accuracy: 0.5217, F1-score: 0.4125\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5989, Domain Loss: 1.1961, Accuracy: 0.5479, F1-score: 0.4500\nEpoch 4/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:05<00:00,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.5357, Domain Loss: 1.2163, Entropy Loss: 2.8718, Accuracy: 0.5654, F1-score: 0.4581\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5273, Domain Loss: 1.2384, Accuracy: 0.5616, F1-score: 0.4611\nModel saved at epoch 4.\nEpoch 5/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.5522, Domain Loss: 1.3282, Entropy Loss: 2.8710, Accuracy: 0.5567, F1-score: 0.4411\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5232, Domain Loss: 1.3169, Accuracy: 0.5571, F1-score: 0.4460\nEpoch 6/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:05<00:00,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.5153, Domain Loss: 1.3610, Entropy Loss: 2.8687, Accuracy: 0.5717, F1-score: 0.4465\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4735, Domain Loss: 1.3346, Accuracy: 0.5708, F1-score: 0.4447\nModel saved at epoch 6.\nEpoch 7/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:05<00:00,  2.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.4852, Domain Loss: 1.3875, Entropy Loss: 2.8659, Accuracy: 0.5750, F1-score: 0.4471\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4217, Domain Loss: 1.3814, Accuracy: 0.5936, F1-score: 0.4707\nEpoch 8/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.4585, Domain Loss: 1.4091, Entropy Loss: 2.8644, Accuracy: 0.6062, F1-score: 0.4829\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5512, Domain Loss: 1.3680, Accuracy: 0.5091, F1-score: 0.4295\nModel saved at epoch 8.\nEpoch 9/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.4324, Domain Loss: 1.3974, Entropy Loss: 2.8623, Accuracy: 0.6286, F1-score: 0.5037\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.3948, Domain Loss: 1.3398, Accuracy: 0.6027, F1-score: 0.4887\nEpoch 10/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.4234, Domain Loss: 1.3965, Entropy Loss: 2.8618, Accuracy: 0.6278, F1-score: 0.5039\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4880, Domain Loss: 1.3719, Accuracy: 0.5274, F1-score: 0.4524\nModel saved at epoch 10.\nEpoch 11/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.4141, Domain Loss: 1.4216, Entropy Loss: 2.8613, Accuracy: 0.6334, F1-score: 0.5173\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4771, Domain Loss: 1.3825, Accuracy: 0.5411, F1-score: 0.4770\nEpoch 12/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3747, Domain Loss: 1.3996, Entropy Loss: 2.8591, Accuracy: 0.6748, F1-score: 0.5764\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4933, Domain Loss: 1.3328, Accuracy: 0.5068, F1-score: 0.4627\nModel saved at epoch 12.\nEpoch 13/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:05<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3464, Domain Loss: 1.3848, Entropy Loss: 2.8567, Accuracy: 0.7004, F1-score: 0.6141\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4467, Domain Loss: 1.3518, Accuracy: 0.5502, F1-score: 0.5028\nEpoch 14/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3405, Domain Loss: 1.3839, Entropy Loss: 2.8560, Accuracy: 0.7093, F1-score: 0.6238\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4252, Domain Loss: 1.3401, Accuracy: 0.5731, F1-score: 0.5302\nModel saved at epoch 14.\nEpoch 15/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3273, Domain Loss: 1.3906, Entropy Loss: 2.8539, Accuracy: 0.7169, F1-score: 0.6307\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4376, Domain Loss: 1.3535, Accuracy: 0.5845, F1-score: 0.5287\nEpoch 16/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3231, Domain Loss: 1.3666, Entropy Loss: 2.8548, Accuracy: 0.7182, F1-score: 0.6284\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5759, Domain Loss: 1.3106, Accuracy: 0.3927, F1-score: 0.3712\nModel saved at epoch 16.\nEpoch 17/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3259, Domain Loss: 1.3643, Entropy Loss: 2.8557, Accuracy: 0.7207, F1-score: 0.6262\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4852, Domain Loss: 1.3271, Accuracy: 0.4863, F1-score: 0.4636\nEpoch 18/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3279, Domain Loss: 1.3906, Entropy Loss: 2.8550, Accuracy: 0.7174, F1-score: 0.6259\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.4968, Domain Loss: 1.3487, Accuracy: 0.4772, F1-score: 0.4562\nModel saved at epoch 18.\nEpoch 19/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3250, Domain Loss: 1.3793, Entropy Loss: 2.8548, Accuracy: 0.7202, F1-score: 0.6242\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.3126, Domain Loss: 1.3298, Accuracy: 0.7009, F1-score: 0.6246\nEpoch 20/20\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/124 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nTraining: 100%|██████████| 124/124 [06:04<00:00,  2.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training - Classification Loss: 2.3204, Domain Loss: 1.3693, Entropy Loss: 2.8543, Accuracy: 0.7200, F1-score: 0.6209\n","output_type":"stream"},{"name":"stderr","text":"Validation:   0%|          | 0/14 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nValidation: 100%|██████████| 14/14 [00:07<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation - Classification Loss: 2.5359, Domain Loss: 1.3339, Accuracy: 0.4315, F1-score: 0.4207\nModel saved at epoch 20.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport os\n\nresults = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Disable gradient calculation for inference\nwith torch.no_grad():\n    # Add a tqdm progress bar to the loop\n    for images, img_paths in tqdm(test_loader, desc=\"Processing Images\", unit=\"image\"):\n        images = images.to(device)\n\n        # Forward pass through the model\n        class_output, domain_output, _ = model(images)\n\n        # Get predicted class labels\n        _, predicted_class = torch.max(class_output, 1)\n        predicted_class = predicted_class.cpu().numpy()[0]  # Convert to numpy\n\n        # Extract the image name without the extension\n        image_name = os.path.splitext(os.path.basename(img_paths[0]))[0]\n\n        # Store the result as (ID, TARGET)\n        results.append({'ID': image_name, 'TARGET': predicted_class})\n\n# Save the results to a CSV file\nresults_df = pd.DataFrame(results)\nresults_df.to_csv('predictions_2.csv', index=False)\n\nprint(\"Predictions saved to predictions_2.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T09:20:40.719127Z","iopub.execute_input":"2024-10-24T09:20:40.719628Z","iopub.status.idle":"2024-10-24T09:40:07.692307Z","shell.execute_reply.started":"2024-10-24T09:20:40.719593Z","shell.execute_reply":"2024-10-24T09:40:07.691241Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Processing Images:   0%|          | 0/15620 [00:00<?, ?image/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/instancenorm.py:88: UserWarning: input's size at dim=0 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\nProcessing Images: 100%|██████████| 15620/15620 [19:26<00:00, 13.39image/s]\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to predictions_2.csv\n","output_type":"stream"}],"execution_count":5}]}