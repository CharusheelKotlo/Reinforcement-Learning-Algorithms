{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Environment : MountainCar-v0\n",
      "State space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space: Discrete(3)\n",
      "Running random agent...\n",
      "Total reward after 21342 steps: -21342.0\n",
      "\n",
      "These are the Rewards(and respective frequencies) after experienting with Random Agent : \n",
      " {-1.0: 21342}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"uncomment it in case gynmasium is not present\"\"\"\n",
    "\n",
    "# %pip install gymnasium[all]\n",
    "# %pip install shimmy\n",
    "# %pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "# %pip install ale_py\n",
    "\n",
    "\"\"\"uncomment it to check if Pong-v0 and others are present or not\"\"\"\n",
    "# # To Check if all modules are working fine \n",
    "# import ale_py\n",
    "# import gymnasium\n",
    "# import shimmy\n",
    "\n",
    "# import gymnasium as gym\n",
    "\n",
    "# req = set(['MountainCar-v0','Pong-v0','CartPole-v0','LunarLander-v2'])\n",
    "# keys = set(gym.envs.registry.keys())\n",
    "# for key in req:\n",
    "\n",
    "#   if(keys.intersection({key}) == set({})):\n",
    "#       print(key +' not found')   \n",
    "\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import shimmy #it is compulsory to import\n",
    "# import ale_py #it's too\n",
    "\n",
    "env_1 = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "print('\\n Environment : MountainCar-v0')\n",
    " # Print state and action space\n",
    "print(\"State space:\", env_1.observation_space)\n",
    "print(\"Action space:\", env_1.action_space)\n",
    "state, _ = env_1.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "step_count = 0\n",
    "rew_1 = {}\n",
    "print(\"Running random agent...\")\n",
    "\n",
    "\n",
    "while not done:\n",
    "    action = env_1.action_space.sample()  # Random action\n",
    "    state, reward, done, _, _ = env_1.step(action)\n",
    "    total_reward += reward\n",
    "    if reward not in rew_1:\n",
    "        rew_1[reward] = 0\n",
    "    rew_1[reward] += 1\n",
    "    step_count += 1\n",
    "\n",
    "print(f\"Total reward after {step_count} steps: {total_reward}\\n\")\n",
    "\n",
    "# Close the environment\n",
    "env_1.close()\n",
    "print(f'These are the Rewards(and respective frequencies) after experienting with Random Agent : \\n {rew_1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "ENVIRONMENT = \"MountainCar-v0\"\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "SAVE_MODELS = True  # Save models to file so you can test later\n",
    "MODEL_PATH = \"./MountainCar-\"  # Models path for saving or loading\n",
    "SAVE_MODEL_INTERVAL = 200 # Save models at every X epoch\n",
    "TRAIN_MODEL = True  # Train model while playing (Make it False when testing a model)\n",
    "\n",
    "LOAD_MODEL_FROM_FILE = False  # Load model from file\n",
    "LOAD_FILE_EPISODE = 0  # Load Xth episode from file\n",
    "\n",
    "BATCH_SIZE = 64  # Minibatch size that select randomly from mem for train nets\n",
    "MAX_EPISODE = 10000  # Max episode\n",
    "MAX_STEP = 1000  # Max step size for one episode\n",
    "\n",
    "MAX_MEMORY_LEN = 100000  # Max memory len\n",
    "MIN_MEMORY_LEN = 10000  # Min memory len before start train \n",
    "\n",
    "GAMMA = 0.999  # Discount rate\n",
    "ALPHA = 0.005  # Learning rate\n",
    "EPSILON_DECAY = 0.999  # Epsilon decay rate by step\n",
    "\n",
    "\n",
    "RENDER_GAME_WINDOW = False # Opens a new window to render the game (Won't work on colab default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNMountain(nn.Module):\n",
    "    def __init__(self,action_space):\n",
    "        super(DQNMountain,self).__init__()\n",
    "        self.fun = nn.Sequential(\n",
    "            nn.Linear(2,16),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(16,action_space)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.fun(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, environment):\n",
    "        \"\"\"\n",
    "        Hyperparameters definition for Agent\n",
    "        \"\"\"\n",
    "\n",
    "        # Activation size for breakout env. Used as output size in network\n",
    "        self.action_size = environment.action_space.n\n",
    "\n",
    "        # Trust rate to our experiences\n",
    "        self.gamma = GAMMA  # Discount coef for future predictions\n",
    "        self.alpha = ALPHA  # Learning Rate\n",
    "\n",
    "        self.epsilon = 1  # Explore or Exploit\n",
    "        self.epsilon_decay = EPSILON_DECAY  # Adaptive Epsilon Decay Rate\n",
    "        self.epsilon_minimum = 0.05  # Minimum for Explore\n",
    "\n",
    "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
    "\n",
    "        # Create two model for DDQN algorithm\n",
    "        self.online_model = DQNMountain(self.action_size).to(DEVICE)\n",
    "        self.target_model = DQNMountain(self.action_size).to(DEVICE) \n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # Adam used as optimizer\n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(),lr=self.alpha,betas=(0.9, 0.999),weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "       \n",
    "\n",
    "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
    "\n",
    "        if act_protocol == 'Explore':\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "                q_values = self.online_model.forward(state)  # (1, action_size)\n",
    "                action = torch.argmax(q_values).item()  # Returns the indices of the maximum value of all elements\n",
    "\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train neural nets with replay memory\n",
    "        returns loss and max_q val predicted from online_net\n",
    "        \"\"\"\n",
    "        if len(self.memory) < MIN_MEMORY_LEN:\n",
    "            loss, max_q = [0, 0]\n",
    "            return loss, max_q\n",
    "        # We get out minibatch and turn it to numpy array\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
    "\n",
    "        # Concat batches in one array\n",
    "        # (np.arr, np.arr) ==> np.BIGarr\n",
    "        state = np.concatenate(state)\n",
    "        next_state = np.concatenate(next_state)\n",
    "\n",
    "        # Convert them to tensors\n",
    "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
    "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
    "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
    "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
    "        # print(state.shape)\n",
    "        # Make predictions\n",
    "        state_q_values = self.online_model(state)\n",
    "        next_states_q_values = self.online_model(next_state)\n",
    "        next_states_target_q_values = self.target_model(next_state)\n",
    "\n",
    "        # Find selected action's q_value\n",
    "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
    "        # Use Bellman function to find expected q value\n",
    "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        loss = criterion(selected_q_value, expected_q_value)\n",
    "\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss, torch.max(state_q_values).item()\n",
    "\n",
    "    def storeResults(self, state, action, reward, nextState, done):\n",
    "        \"\"\"\n",
    "        Store every result to memory\n",
    "        \"\"\"\n",
    "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
    "\n",
    "    def adaptiveEpsilon(self):\n",
    "        \"\"\"\n",
    "        Adaptive Epsilon means every step\n",
    "        we decrease the epsilon so we do less Explore\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|\u001b[32m          \u001b[0m| 68/9999 [01:24<3:25:56,  1.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state  \u001b[38;5;66;03m# Update state\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_MODEL:\n\u001b[1;32m---> 48\u001b[0m     loss, max_q_val \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     loss, max_q_val \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[1;32mIn[9], line 83\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(selected_q_value, expected_q_value)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 83\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, torch\u001b[38;5;241m.\u001b[39mmax(state_q_values)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\charu\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\charu\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\charu\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "environment = gym.make(ENVIRONMENT)  # Get env\n",
    "agent = Agent(environment)  # Create Agent\n",
    "l = []\n",
    "if LOAD_MODEL_FROM_FILE:\n",
    "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pth\"))\n",
    "\n",
    "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
    "        param = json.load(outfile)\n",
    "        agent.epsilon = param.get('epsilon')\n",
    "\n",
    "    startEpisode = LOAD_FILE_EPISODE + 1\n",
    "\n",
    "else:\n",
    "    startEpisode = 1\n",
    "\n",
    "last_100_ep_reward = deque(maxlen=100)  \n",
    "total_step = 1\n",
    "avg_reward_list = []\n",
    "avg_reward = 0\n",
    "best_avg_reward=-1000 \n",
    "\n",
    "for episode in tqdm(range(startEpisode, MAX_EPISODE),colour='green'):\n",
    "\n",
    "    state,_ = environment.reset()  # Reset env\n",
    "   \n",
    "\n",
    "    total_max_q_val = 0\n",
    "    total_reward = 0 \n",
    "    # total_loss = 0  \n",
    "    done = False\n",
    "    while not done and total_reward>-1000:\n",
    "\n",
    "        if RENDER_GAME_WINDOW:\n",
    "            environment.render()\n",
    "\n",
    "        # Select and perform an action\n",
    "        action = agent.act(state)  # Act\n",
    "        next_state, reward, done,_,_ = environment.step(action)  \n",
    "        \n",
    "        \n",
    "        agent.storeResults(state, action, reward, next_state, done)  \n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state  # Update state\n",
    "\n",
    "        if TRAIN_MODEL:\n",
    "            loss, max_q_val = agent.train()  \n",
    "        else:\n",
    "            loss, max_q_val = [0, 0]\n",
    "\n",
    "        # total_loss += loss\n",
    "        total_max_q_val += max_q_val\n",
    "        total_reward += reward\n",
    "        total_step += 1\n",
    "        if total_step % 1000 == 0:\n",
    "            agent.adaptiveEpsilon()  \n",
    "\n",
    "        if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0: \n",
    "            weightsPath = MODEL_PATH + str(episode) + '.pth'\n",
    "            epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
    "\n",
    "            torch.save(agent.online_model.state_dict(), weightsPath)\n",
    "            with open(epsilonPath, 'w') as outfile:\n",
    "                json.dump({'epsilon': agent.epsilon}, outfile)\n",
    "\n",
    "        if TRAIN_MODEL and total_step%10000==0:\n",
    "            agent.target_model.load_state_dict(agent.online_model.state_dict())  \n",
    "\n",
    "        last_100_ep_reward.append(total_reward)\n",
    "\n",
    "        # if(episode%100==0):\n",
    "        \n",
    "        avg_max_q_val = total_max_q_val\n",
    "        \n",
    "    l.append(total_reward)\n",
    "\n",
    "    avg_reward_list.append(np.mean(np.array(list(last_100_ep_reward))))\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        plt.plot(l)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Tot_rew')\n",
    "        plt.title(f'Plot at Episode {episode}')\n",
    "        plt.savefig(f'Mountain_carv0.png') \n",
    "        plt.close()\n",
    "\n",
    "        plt.plot(avg_reward_list)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Tot_rew')\n",
    "        plt.title(f'best avg {episode}')\n",
    "        plt.savefig(f'Mountain_carv0_best_avg.png') \n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charu\\AppData\\Local\\Temp\\ipykernel_3852\\1808469061.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('MountainCar-6000.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('MountainCar-v0',render_mode = 'human')\n",
    "state,_ = env.reset()\n",
    "model = DQNMountain(3)\n",
    "model.load_state_dict(torch.load('MountainCar-6000.pth'))\n",
    "model.eval()\n",
    "tore = 0\n",
    "while not done:\n",
    "    action = model(torch.FloatTensor(state)).argmax().item()  # Random action\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    tore += reward\n",
    "env.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charu\\AppData\\Local\\Temp\\ipykernel_12292\\1888278600.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_dqn.load_state_dict(torch.load(r\"moun_best\\MountainCar-6000.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-176.0\n",
      "-164.0\n",
      "-250.0\n",
      "-259.0\n",
      "-251.0\n"
     ]
    }
   ],
   "source": [
    "env2 = gym.make('MountainCar-v0', render_mode='human')\n",
    "\n",
    "test_dqn = DQNMountain(3) \n",
    "test_dqn.load_state_dict(torch.load(r\"moun_best\\MountainCar-6000.pth\"))\n",
    "test_dqn.eval() \n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    state = env2.reset()[0]  # Initialize to state 0\n",
    "    terminated = False      # True when agent falls in hole or reached goal\n",
    "    truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "    rewards = 0\n",
    "\n",
    "    # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "    while(not terminated and rewards>-300):  \n",
    "        # Select best action   \n",
    "        with torch.no_grad():\n",
    "            action = test_dqn(torch.tensor(state)).argmax().item()\n",
    "\n",
    "        # Execute action\n",
    "        state,reward,terminated,truncated,_ = env2.step(action)\n",
    "        rewards += reward\n",
    "\n",
    "    print(rewards)\n",
    "\n",
    "env2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
